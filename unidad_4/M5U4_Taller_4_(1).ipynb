{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6vF8WHtOmqKG",
      "metadata": {
        "id": "6vF8WHtOmqKG"
      },
      "source": [
        "<img src = \"https://drive.google.com/uc?export=view&id=1VV2e_u46fNm_ewns8QW2HGRZAPHh-e2t\" alt = \"Encabezado MLDS\" width = \"100%\">  </img>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca9e82b0",
      "metadata": {
        "id": "ca9e82b0"
      },
      "source": [
        "# **Taller 4 - *Transformers***\n",
        "---\n",
        "\n",
        "En este taller usted tendrá que usar un modelo de Transformer de [Hugginface](https://huggingface.co/) para hacer un proceso de *Fine Tunning*. El modelo final será un clasificador de sarcasmo. Usaremos un conjunto de datos que consiste en encabezados de periódicos (textos en inglés).\n",
        "\n",
        "Importemos las librerías requeridas para solucionar la tarea:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ugzb1JKXAZHe",
      "metadata": {
        "id": "Ugzb1JKXAZHe"
      },
      "source": [
        "> **Importante:  Recomendamos utilizar GPU para la ejecución de este notebook, ya que puede tomar mucho tiempo la ejeución de algunos casos de prueba en caso de que no se utilice.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gRusQKhlAd7j",
      "metadata": {
        "id": "gRusQKhlAd7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbbbbb0f-0cba-44e3-e97c-63680f7b401e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rlxcrypt in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: imphook in /usr/local/lib/python3.10/dist-packages (from rlxcrypt) (1.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from rlxcrypt) (3.0.10)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from rlxcrypt) (3.20.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from rlxcrypt) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->rlxcrypt) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->rlxcrypt) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install rlxcrypt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5z-Y0-O6AfG9",
      "metadata": {
        "id": "5z-Y0-O6AfG9"
      },
      "outputs": [],
      "source": [
        "!wget --no-cache -O session.pye -q https://raw.githubusercontent.com/JuezUN/INGInious/master/external%20libs/session.pye"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MNycOhRkAggS",
      "metadata": {
        "id": "MNycOhRkAggS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "ce27be18-b41a-46ca-d08b-d129b1bcaf96"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "    <div style=\"display: flex\">\n",
              "        <div style=\"padding: 10px\">\n",
              "          <img\n",
              "            src=\"https://uncode.unal.edu.co/static/images/logo_unc.svg\"\n",
              "            width=\"50px\"\n",
              "            height=\"50px\"\n",
              "          />\n",
              "        </div>\n",
              "      \n",
              "        <div>\n",
              "          <h2>UNCode Notebooks grader API</h2>\n",
              "          <hr />\n",
              "        </div>\n",
              "      </div>\n",
              "      \n",
              "      <div style=\"display: flex\">\n",
              "        <span>Please enter authentication data</span>\n",
              "      </div>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your UNCode username: jumrodriguezba\n",
            "Please enter your password: ··········\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h5 style=\"color: green\"><Response [200]></h5>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h5 style=\"color: green\">Auth process succeeded</h5>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h5>View your courses <a href=\"https://uncode.unal.edu.co/courselist\">at UNCode platform</a></h5>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>You will sumbit code to course: <span style=\"color: green\">Deep learning: Introducción al aprendizaje profundo con Python | Grupo MLDS-5 | 2024-1<span></h3>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>You will sumbit code to task: <span style=\"color: blue\">Taller 4: Transformers<span></h3>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import rlxcrypt\n",
        "import session\n",
        "\n",
        "grader = session.LoginSequence(\"DLIAAPCP-GroupMLDS-5-2024-2024-1@6ef4c2a7-c41d-42bc-b6f5-aeb0b96f9329\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install tf-keras"
      ],
      "metadata": {
        "id": "R5yiXBShPUG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98937feb-0477-4874-ab4e-b555f4803b27"
      },
      "id": "R5yiXBShPUG3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.39.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: tf-keras in /usr/local/lib/python3.10/dist-packages (2.15.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tf-keras) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15->tf-keras) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1' # Establece la versión legacy"
      ],
      "metadata": {
        "id": "h0pZ-h_ZPbVG"
      },
      "id": "h0pZ-h_ZPbVG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_tbd6UioOken",
      "metadata": {
        "id": "_tbd6UioOken"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import transformers\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OHuWA9eJrzVU",
      "metadata": {
        "id": "OHuWA9eJrzVU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf5b224-9bd0-4064-9459-ebc49ea3c9b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "Numpy 1.25.2\n",
            "Transformers 4.39.3\n",
            "Tensorflow 2.15.0\n"
          ]
        }
      ],
      "source": [
        "# Versiones de las librerías usadas.\n",
        "!python --version\n",
        "print('Numpy', np.__version__)\n",
        "print('Transformers', transformers.__version__)\n",
        "print('Tensorflow', tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aPNlud8Or71E",
      "metadata": {
        "id": "aPNlud8Or71E"
      },
      "source": [
        "Esta actividad se realizó con las siguientes versiones:\n",
        "*  Python 3.9.16\n",
        "*  Numpy 1.22.4\n",
        "*  Transformers 4.28.1\n",
        "*  Tensorflow 2.12.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aed3ce8",
      "metadata": {
        "id": "3aed3ce8"
      },
      "source": [
        "## **Cargar Datos**\n",
        "---\n",
        "\n",
        "El conjunto de datos esta compuesto por titulares de periódicos. Los datos fueron preparados por _Laurence Moroney_ (código fuente disponible en [GitHub](https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%203%20-%20Lesson%202c.ipynb))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qlFk1VOKprO6",
      "metadata": {
        "id": "qlFk1VOKprO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c613333-70e1-4fcc-a4ad-50f04fc3833f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-17 23:26:20--  https://raw.githubusercontent.com/mindlab-unal/mlds5-dataset-unit4-Tarea4-Transformers/main/sarcasm.json?raw=true\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5616834 (5.4M) [text/plain]\n",
            "Saving to: ‘sarcasm.json’\n",
            "\n",
            "sarcasm.json        100%[===================>]   5.36M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-04-17 23:26:21 (194 MB/s) - ‘sarcasm.json’ saved [5616834/5616834]\n",
            "\n",
            "26709\n"
          ]
        }
      ],
      "source": [
        "!wget -O sarcasm.json https://raw.githubusercontent.com/mindlab-unal/mlds5-dataset-unit4-Tarea4-Transformers/main/sarcasm.json?raw=true\n",
        "with open(\"sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)\n",
        "sentences = []\n",
        "labels = []\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(item['is_sarcastic'])\n",
        "print(len(sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sAPABEWoqLHp",
      "metadata": {
        "id": "sAPABEWoqLHp"
      },
      "source": [
        "Tenemos 26709 muestras. Démosle un vistazo a los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XDwPKeVvqNyE",
      "metadata": {
        "id": "XDwPKeVvqNyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff7bbd6-5bf2-4ec9-dc0c-40cf7a32ea0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label \t sentence \n",
            " ----\t----------\n",
            "0 \t on alan turing, me and my son \n",
            "\n",
            "0 \t los angeles mayor pledges $138 million to help the biggest homeless population in the u.s. \n",
            "\n",
            "1 \t nra says mass shootings just the unfortunate price of protecting people's freedom to commit mass shootings \n",
            "\n",
            "0 \t gina rodriguez responds to golden globes' america ferrera mix-up \n",
            "\n",
            "0 \t for trump, words are stupid things \n",
            "\n",
            "0 \t meth production in illinois sees decrease after four-year increase \n",
            "\n",
            "1 \t report: mom has plan for tub of whipped cream in fridge \n",
            "\n",
            "0 \t stop the madness \n",
            "\n",
            "0 \t malcolm-jamal warner likens cosby scandal to woody allen, roman polanski controversies \n",
            "\n",
            "0 \t listen to the roots' 'tomorrow' today \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('label','\\t','sentence','\\n','----\\t----------')\n",
        "for i in np.random.choice(26709, 10, replace=False):\n",
        "  print(labels[i],'\\t',sentences[i],'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nMqdRuiQtXf_",
      "metadata": {
        "id": "nMqdRuiQtXf_"
      },
      "source": [
        "La etiqueta `0` corresponde a _no sarcasmo_ y la `1` a _sarcasmo_. Es decir, _sarcasmo_ es la clase positiva."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04054323",
      "metadata": {
        "id": "04054323"
      },
      "source": [
        "## **Particiones de entrenamiento y prueba**\n",
        "---\n",
        "Como siempre, es importante separar una parte de los datos que no serán usados en el entrenamiento para evaluar el desempeño del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06264f5e",
      "metadata": {
        "id": "06264f5e"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(sentences, labels, test_size=0.2, stratify = labels, random_state = 30)\n",
        "X_train, X_val,  y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify = y_temp, random_state = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FwL41WOci0_k",
      "metadata": {
        "id": "FwL41WOci0_k"
      },
      "source": [
        "Hacemos una codificación _one hot_ de las etiquetas de entrenamiento y validación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "InJpDpJSBaKe",
      "metadata": {
        "id": "InJpDpJSBaKe"
      },
      "outputs": [],
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_val = tf.keras.utils.to_categorical(y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5SDtEX4Ki-Om",
      "metadata": {
        "id": "5SDtEX4Ki-Om"
      },
      "source": [
        "Y contamos cuántos textos hay en cada partición:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cVu3tqJIT0XB",
      "metadata": {
        "id": "cVu3tqJIT0XB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d2ff02-2102-4e89-aa49-2d258dd94274"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16025, 5342, 5342)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(X_train), len(X_val), len(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gi_p-YIjxG8-",
      "metadata": {
        "id": "Gi_p-YIjxG8-"
      },
      "source": [
        "Tenemos entonces 16025 muestras para entrenamiento, 5342 para validación y la misma cantidad para prueba."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OV-aLBdExviM",
      "metadata": {
        "id": "OV-aLBdExviM"
      },
      "source": [
        "# **DistilBERT**\n",
        "----\n",
        "[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) es un modelo Transformer pequeño, rápido, poco costoso y ligero, entrenado a partir BERT, con una técnica llamada [_Destilación de conocimiento_](https://en.wikipedia.org/wiki/Knowledge_distillation).\n",
        "\n",
        "La destilación de conocimiento es el proceso de transferencia de conocimiento de un modelo grande a otro más pequeño. Aunque los modelos grandes (como BERT) tienen mayor capacidad de aprendizaje que los modelos pequeños, es posible que esta capacidad no se utilice plenamente. Evaluar un modelo puede ser igual de costoso desde el punto de vista computacional aunque utilice muy poco de su capacidad de conocimiento. La destilación de conocimiento transfiere conocimientos de un modelo grande a otro más pequeño sin pérdida de validez. Como los modelos más pequeños son menos costosos de evaluar, se pueden implantar en hardware menos potente (como un dispositivo móvil, o en este caso, un entorno de Colab)\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=15uxvIrnqiJJmkqhet_eLGud7CKlQ-145\" alt =\"DistilBERT\" width=\"80%\" /></center>\n",
        "\n",
        "DistilBERT tiene un 40% menos de parámetros que el modelo BERT, se ejecuta un 60% más rápido y conserva más del 95% del rendimiento de BERT. Originalmente se propuso en el artículo: [_DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter_](https://arxiv.org/abs/1910.01108). Para nuestra fortuna, el modelo se encuentra implementado y disponible en HuggingFace: https://huggingface.co/docs/transformers/model_doc/distilbert.\n",
        "\n",
        "Vamos entonces a aplicar lo aprendido en la unidad. Recordemos cuál es el pipeline estandar de ejecución de un Transformer:\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1yaxcm5ceXkGuMZ5u6iQaZ0sGyYXGU3Pu\" alt =\"Gráfico ilustrativo de la pipeline de ejecución\" width=\"80%\" /></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TTzBI8m2ki_2",
      "metadata": {
        "id": "TTzBI8m2ki_2"
      },
      "source": [
        "> **La tarea es incremental, por lo tanto es recomendable resolver los puntos en orden.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "119d73dc",
      "metadata": {
        "id": "119d73dc"
      },
      "source": [
        "# **1. Tokenización**\n",
        "---\n",
        "Aplique el preprocesamiento requerido para usar una arquitectura DistilBERT sobre los conjuntos definidos de entrenamiento, validación y prueba. Complete la función `tokenize`, que recibe como argumentos un conjnuto de datos y los parámetros necesarios para tokenizarlos. La función debe retornar un objeto tipo `BatchEncoding` como lo vimos en el taller guiado. Utilice `AutoTokenizer` y la tokenización del modelo `\"distilbert-base-uncased\"`.\n",
        "\n",
        "**Entradas**:\n",
        "\n",
        "* **`model_name`**: un `str` que representa el nombre del tokenizer del modelo pre-entrenado a definir.\n",
        "* **`X`**, `list`, una lista de secuencuas de texto.\n",
        "* **`truncate`**, `boolean`, variable booleana para definir si los textos se truncan o no.  \n",
        "* **`padd`**, `boolean`, variable booleana para definir si los textos se rellenan o no.\n",
        "* **`tensor`**, un `str` que puede ser `np`, `tf` o `pt` para indicar el tipo de tensor que debe devolver el tokenizador.\n",
        "\n",
        "**Salida**:\n",
        "\n",
        "* **`encodings`**: un objeto tipo `BatchEncoding` para tokenizar textos para `DistilBERT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z3DHSL8_QT-T",
      "metadata": {
        "id": "Z3DHSL8_QT-T"
      },
      "outputs": [],
      "source": [
        "# FUNCION CALIFICADA tokenize\n",
        "from transformers import AutoTokenizer\n",
        "def tokenize(model_name, X, truncate, padd, tensor):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    encodings = tokenizer(X, truncation=truncate, padding=padd, return_tensors=tensor)\n",
        "    return encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zb-y2Ngz1LdS",
      "metadata": {
        "id": "zb-y2Ngz1LdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "badadc9b-24a5-4497-c23d-4935d3bd89fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding tipo: <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "Primeros cinco tokens de test: [ 101 5712 2022 4974 2075]\n"
          ]
        }
      ],
      "source": [
        "# TEST_CELL\n",
        "train_encodings = tokenize(model_name='distilbert-base-uncased',\n",
        "                           X = X_train,\n",
        "                           truncate=True,\n",
        "                           padd=True,\n",
        "                           tensor= \"np\")\n",
        "val_encodings = tokenize(model_name='distilbert-base-uncased',\n",
        "                         X = X_val,\n",
        "                         truncate=True,\n",
        "                         padd=True,\n",
        "                         tensor= \"np\")\n",
        "test_encodings = tokenize(model_name='distilbert-base-uncased',\n",
        "                          X = X_test,\n",
        "                          truncate=True,\n",
        "                          padd=True,\n",
        "                          tensor= \"np\")\n",
        "\n",
        "print(\"Encoding tipo:\", type(train_encodings))\n",
        "print(\"Primeros cinco tokens de test:\", test_encodings['input_ids'][0][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W3n6wvGxRjaz",
      "metadata": {
        "id": "W3n6wvGxRjaz"
      },
      "source": [
        "**Salida esperada:**\n",
        "```\n",
        "Encoding tipo: <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
        "Primeros cinco tokens de test: [ 101 5712 2022 4974 2075]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zyKTa9oJEaHy",
      "metadata": {
        "id": "zyKTa9oJEaHy"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Uvm6y5zHQzZC",
      "metadata": {
        "id": "Uvm6y5zHQzZC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "26b26ce4-8a07-4c4f-dd03-753e4e227a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1_1_1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Test Run correctly</h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h6 style=\"color: green;\">Your test grade: 100</h6>\n",
              "<h6>Feedback: Probando código ejemplo.</h6>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "grader.run_test(\"Test 1_1_1\", globals())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9_fvBC0dW8m8",
      "metadata": {
        "id": "9_fvBC0dW8m8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "624d4993-4a17-4ea6-f781-f027f538f2d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1_1_2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Test Run correctly</h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h6 style=\"color: green;\">Your test grade: 100</h6>\n",
              "<h6>Feedback: Probando código.</h6>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "grader.run_test(\"Test 1_1_2\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89599031",
      "metadata": {
        "id": "89599031"
      },
      "source": [
        "# **2. _Fine Tuning_ con _DistilBERT_**\n",
        "---\n",
        "Vamos a llevar a cabo el flujo de trabajo neesario para hacer _Fine Tuning_ con un modelo preentrenado."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VSDnmpD_t-gv",
      "metadata": {
        "id": "VSDnmpD_t-gv"
      },
      "source": [
        "## **2.1 Definir el modelo**\n",
        "---\n",
        "Primero vamos a definir el modelo. Implemente una función llamada `pretrained_model` que reciba como entrada un nombre de modelo y retorne una instancia de ese modelo pre-entrenado utilizando la biblioteca Transformers de HuggingFace. La función debe utilizar `TFAutoModelForSequenceClassification` para cargar el modelo y devolverlo.\n",
        "\n",
        "**Entradas**:\n",
        "\n",
        "* **`model_name`**: un `str` que representa el nombre del modelo pre-entrenado a cargar.\n",
        "\n",
        "**Salida**:\n",
        "\n",
        "* **`model`**: una instancia del modelo pre-entrenado cargado utilizando `TFAutoModelForSequenceClassification`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4EYskRy4ADjt",
      "metadata": {
        "id": "4EYskRy4ADjt"
      },
      "outputs": [],
      "source": [
        "# FUNCION CALIFICADA pretrained_model\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "def pretrained_model(model_name):\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QBRQVMIdBvJi",
      "metadata": {
        "id": "QBRQVMIdBvJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49adae2-6c86-4755-f1ac-8c2eb07ddff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification'>\n",
            "Model: \"tf_distil_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  66362880  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        multiple                  0 (unused)\n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66955010 (255.41 MB)\n",
            "Trainable params: 66955010 (255.41 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# TEST_CELL\n",
        "model = pretrained_model('distilbert-base-uncased')\n",
        "print(type(model))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "610N2ALPBkqZ",
      "metadata": {
        "id": "610N2ALPBkqZ"
      },
      "source": [
        "**Salida esperada:**\n",
        "```\n",
        "<class 'transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification'>\n",
        "Model: \"tf_distil_bert_for_sequence_classification\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " distilbert (TFDistilBertMai  multiple                 66362880  \n",
        " nLayer)                                                         \n",
        "                                                                 \n",
        " pre_classifier (Dense)      multiple                  590592    \n",
        "                                                                 \n",
        " classifier (Dense)          multiple                  1538      \n",
        "                                                                 \n",
        " dropout_19 (Dropout)        multiple                  0         \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 66,955,010\n",
        "Trainable params: 66,955,010\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "None\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Otg-YvCucLdY",
      "metadata": {
        "id": "Otg-YvCucLdY"
      },
      "source": [
        "### **Nota importante**\n",
        "---\n",
        "Veamos detalladamente la arquitectura del modelo anterior. Tenemos 66,955,010 de parámetros repartidos en cuatro bloques. El primer bloque es el modelo base `distilbert`. Los parámetros de ese bloque (66,362,880) son los que se deben congelar al momento de hacer el calentamiento o _warming up_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2xh-TZB8D0kf",
      "metadata": {
        "id": "2xh-TZB8D0kf"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zq5F7gD8pOaG",
      "metadata": {
        "id": "zq5F7gD8pOaG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "c0e24d92-9dd1-4096-e083-763cb17d07a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 2_1_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Test Run correctly</h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h6 style=\"color: green;\">Your test grade: 100</h6>\n",
              "<h6>Feedback: Probando código ejemplo.</h6>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "grader.run_test(\"Test 2_1_1\", globals())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "psjh0i7FrXQB",
      "metadata": {
        "id": "psjh0i7FrXQB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "1bc16569-a7ab-43b1-e84a-9ef886cf548f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 2_1_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Test Run correctly</h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h6 style=\"color: green;\">Your test grade: 100</h6>\n",
              "<h6>Feedback: Probando código ejemplo.</h6>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "grader.run_test(\"Test 2_1_2\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AY2YRFzhUE8a",
      "metadata": {
        "id": "AY2YRFzhUE8a"
      },
      "source": [
        "## **2.2 Compilar**\n",
        "---\n",
        "\n",
        "Igual que con BERT, DistilBERT se define por defecto con dos neuronas de salida con activación linea. Complete entonces la función `compile` que prepara el modelo para el entrenamiento. La función debe recibir como entrada un modelo, un optimizador y una tasa de aprendizaje, y luego compila el modelo con una función de pérdida `CategoricalCrossentropy`.\n",
        "\n",
        "**Entradas:**\n",
        "\n",
        "*    **`model`**: una instancia del modelo a entrenar tipo `TFAutoModelForSequenceClassification`.\n",
        "*    **`optimizer`**: una instancia del optimizador de tipo `keras.optimizers` a utilizar durante el entrenamiento.\n",
        "*    **`l_r`**: `float`, un número flotante que representa la tasa de aprendizaje.\n",
        "\n",
        "**Salida:**\n",
        "\n",
        "* **`model`**: una instancia compilada del modelo tipo `TFAutoModelForSequenceClassification`.\n",
        "\n",
        "> **Notas:**\n",
        "  * Utilice el método `.assign()` para ajustar la tasa de aprendizaje del optimizador al valor proporcionado.\n",
        "  * Recuerde definir `from_logits=True` dentro de la función de pérdida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SLtRWuN5A1N9",
      "metadata": {
        "id": "SLtRWuN5A1N9"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN CALIFICADA compile_model\n",
        "\n",
        "def compile_model(model, optimizer, l_r):\n",
        "    optimizer.learning_rate.assign(l_r)\n",
        "\n",
        "    # Compilar el modelo con la función de pérdida CategoricalCrossentropy\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cEv0ylZTBiDs",
      "metadata": {
        "id": "cEv0ylZTBiDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91aa2ecf-d3e0-4f5e-d3d7-383818f546a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'optimizer': {'module': 'keras.optimizers',\n",
              "  'class_name': 'Adam',\n",
              "  'config': {'name': 'Adam',\n",
              "   'weight_decay': None,\n",
              "   'clipnorm': None,\n",
              "   'global_clipnorm': None,\n",
              "   'clipvalue': None,\n",
              "   'use_ema': False,\n",
              "   'ema_momentum': 0.99,\n",
              "   'ema_overwrite_frequency': None,\n",
              "   'jit_compile': True,\n",
              "   'is_legacy_optimizer': False,\n",
              "   'learning_rate': 0.0010000000474974513,\n",
              "   'beta_1': 0.9,\n",
              "   'beta_2': 0.999,\n",
              "   'epsilon': 1e-07,\n",
              "   'amsgrad': False},\n",
              "  'registered_name': None},\n",
              " 'loss': {'module': 'keras.losses',\n",
              "  'class_name': 'CategoricalCrossentropy',\n",
              "  'config': {'reduction': 'auto',\n",
              "   'name': 'categorical_crossentropy',\n",
              "   'from_logits': True,\n",
              "   'label_smoothing': 0.0,\n",
              "   'axis': -1,\n",
              "   'fn': 'categorical_crossentropy'},\n",
              "  'registered_name': None},\n",
              " 'metrics': None,\n",
              " 'loss_weights': None,\n",
              " 'weighted_metrics': None,\n",
              " 'run_eagerly': None,\n",
              " 'steps_per_execution': None,\n",
              " 'jit_compile': None}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# TEST_CELL\n",
        "model = compile_model(model=model,\n",
        "                   optimizer=tf.keras.optimizers.Adam(),\n",
        "                   l_r=1e-3\n",
        "                   )\n",
        "model.get_compile_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "je6S-JHMBnGB",
      "metadata": {
        "id": "je6S-JHMBnGB"
      },
      "source": [
        "**Salida esperada:**\n",
        "```\n",
        "{'optimizer': {'module': 'keras.optimizers',\n",
        "  'class_name': 'Adam',\n",
        "  'config': {'name': 'Adam',\n",
        "   'weight_decay': None,\n",
        "   'clipnorm': None,\n",
        "   'global_clipnorm': None,\n",
        "   'clipvalue': None,\n",
        "   'use_ema': False,\n",
        "   'ema_momentum': 0.99,\n",
        "   'ema_overwrite_frequency': None,\n",
        "   'jit_compile': True,\n",
        "   'is_legacy_optimizer': False,\n",
        "   'learning_rate': 0.0010000000474974513,\n",
        "   'beta_1': 0.9,\n",
        "   'beta_2': 0.999,\n",
        "   'epsilon': 1e-07,\n",
        "   'amsgrad': False},\n",
        "  'registered_name': None},\n",
        " 'loss': {'module': 'keras.losses',\n",
        "  'class_name': 'CategoricalCrossentropy',\n",
        "  'config': {'reduction': 'auto',\n",
        "   'name': 'categorical_crossentropy',\n",
        "   'from_logits': True,\n",
        "   'label_smoothing': 0.0,\n",
        "   'axis': -1},\n",
        "  'registered_name': None},\n",
        " 'metrics': None,\n",
        " 'loss_weights': None,\n",
        " 'weighted_metrics': None,\n",
        " 'run_eagerly': None,\n",
        " 'steps_per_execution': None,\n",
        " 'jit_compile': None}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vMjJCQgPD8c4",
      "metadata": {
        "id": "vMjJCQgPD8c4"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lTHOp-tvgjMl",
      "metadata": {
        "id": "lTHOp-tvgjMl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "9d2c1a09-af85-447d-da56-dc29c64b2fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 2_2_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Test Run correctly</h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h6 style=\"color: green;\">Your test grade: 100</h6>\n",
              "<h6>Feedback: Probando código ejemplo.</h6>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "grader.run_test(\"Test 2_2_1\", globals())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_0yWvCVZhcuD",
      "metadata": {
        "id": "_0yWvCVZhcuD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "409a7d8e-766d-4472-8965-f579568b1a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 2_2_2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Test Run correctly</h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h6 style=\"color: green;\">Your test grade: 100</h6>\n",
              "<h6>Feedback: Probando código ejemplo.</h6>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "grader.run_test(\"Test 2_2_2\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f78f7424",
      "metadata": {
        "id": "f78f7424"
      },
      "source": [
        "## **2.3 Función de entrenamiento**\n",
        "---\n",
        "Complete la función llamada `train_model` que entrena un modelo utilizando un conjunto de datos de entrenamiento. La función debe recibir como un modelo, los datos de entrenamiento, el número de épocas y el tamaño del _batch, y devolver el modelo entrenado.\n",
        "\n",
        "**Entradas**:\n",
        "\n",
        "*    **`model`**: una instancia del modelo a entrenar tipo `TFAutoModelForSequenceClassification`\n",
        "*    **`X_train`**: un objeto tipo `BatchEncoding` para tokenizar textos.\n",
        "*    **`y_train`**: un arreglo de numpy `np.array` que representa las etiquetas de los datos de entrenamiento.\n",
        "*    **`X_val`**: un objeto tipo `BatchEncoding` para tokenizar textos.\n",
        "*    **`y_val`**: un arreglo de numpy `np.array` que representa las etiquetas de los datos de validación.\n",
        "*    **`epochs`**: `int`, un número entero que representa el número de épocas de entrenamiento.\n",
        "*    **`batch_size`**: `int`, un número entero que representa el tamaño del _batch_.\n",
        "*    **`train_base`**: `boolean`, una variable booleana para definir si se congelan o no las capas del modelo base (dependiendo si se quiere hacer _warming up_ o _fine tuning_).\n",
        "\n",
        "**Salida**:\n",
        "\n",
        "*  **`history`**: un objeto tipo `History` de tensorflow con la información del entrenamiento del modelo.\n",
        "*    **`model`**: una instancia del modelo entrenado tipo `TFAutoModelForSequenceClassification`\n",
        "\n",
        "Para implementar la función, utilice un ciclo `for` para controlar el atributo `trainable` de las capas del modelo, excepto la capa de salida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jYG9FJiVJSpP",
      "metadata": {
        "id": "jYG9FJiVJSpP"
      },
      "outputs": [],
      "source": [
        "# FUNCION CALIFICADA train_model\n",
        "def train_model(model,\n",
        "                    X_train, y_train,\n",
        "                    X_val, y_val,\n",
        "                    epochs,\n",
        "                    batch_size,\n",
        "                    train_base):\n",
        "    for i in range(len(model.layers)-3):\n",
        "        model.layers[i].trainable = train_base\n",
        "    history = model.fit(dict(X_train), y_train,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=(dict(X_val), y_val))\n",
        "    return history, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "heBQq44nBxHe",
      "metadata": {
        "id": "heBQq44nBxHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d303fd-07b7-495b-ef7b-0adf378c9f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x78ff06de9ea0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x78ff06de9ea0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "501/501 [==============================] - 174s 246ms/step - loss: 0.6913 - val_loss: 0.6857\n",
            "Model: \"tf_distil_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  66362880  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66955010 (255.41 MB)\n",
            "Trainable params: 592130 (2.26 MB)\n",
            "Non-trainable params: 66362880 (253.15 MB)\n",
            "_________________________________________________________________\n",
            "None\n",
            "dict_keys(['loss', 'val_loss'])\n",
            "El modelo se ha entrenado durante 1 epoch\n"
          ]
        }
      ],
      "source": [
        "# TEST_CELL\n",
        "history, model = train_model(model=model,\n",
        "                             X_train=train_encodings,\n",
        "                             y_train=y_train,\n",
        "                             X_val=val_encodings,\n",
        "                             y_val=y_val,\n",
        "                             epochs=1,\n",
        "                             batch_size=32,\n",
        "                             train_base = False)\n",
        "print(model.summary())\n",
        "print(history.history.keys())\n",
        "print('El modelo se ha entrenado durante',len(history.history['val_loss']),'epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y2biuU8PBo7j",
      "metadata": {
        "id": "Y2biuU8PBo7j"
      },
      "source": [
        "**Salida esperada:**\n",
        "```\n",
        "Model: \"tf_distil_bert_for_sequence_classification\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " distilbert (TFDistilBertMai  multiple                 66362880  \n",
        " nLayer)                                                         \n",
        "                                                                 \n",
        " pre_classifier (Dense)      multiple                  590592    \n",
        "                                                                 \n",
        " classifier (Dense)          multiple                  1538      \n",
        "                                                                 \n",
        " dropout_19 (Dropout)        multiple                  0         \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 66,955,010\n",
        "Trainable params: 592,130\n",
        "Non-trainable params: 66,362,880\n",
        "_________________________________________________________________\n",
        "None\n",
        "dict_keys(['loss', 'val_loss'])\n",
        "El modelo se ha entrenado durante 1 epoch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AJKn_ykLD-mI",
      "metadata": {
        "id": "AJKn_ykLD-mI"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hILzmKwlFgqa",
      "metadata": {
        "id": "hILzmKwlFgqa"
      },
      "source": [
        "> Tiempo estimado: 2:10:00 h sin GPU, 02:35 m con GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ph6cYHHWtZeE",
      "metadata": {
        "id": "ph6cYHHWtZeE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "bfdd5f37-4f00-4079-f6bc-615a5498d7e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 2_3_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Test Run correctly</h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h6 style=\"color: green;\">Your test grade: 100</h6>\n",
              "<h6>Feedback: Probando código ejemplo.</h6>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "grader.run_test(\"Test 2_3_1\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc02068",
      "metadata": {
        "id": "7bc02068"
      },
      "source": [
        "## **2.4 _Fine Tuning_**\n",
        "---\n",
        "Ahora vamos a usar las funciones que definió anteriormente para hacer todo el proceso de ajuste fino, comenzando por hacer un calentamiento o _warming up_ entranando solo la capa final del modelo, y luego entrenando el modelo completo.\n",
        "\n",
        "Complete la función `fine_tuning`. Esta función recible los datos de entrenamiento y valdación, la tasa de aprendizaje del _warming up_ y la tasa de aprendizaje del _fine tuning_, así como las epochs dedicadas a cada etapa del entrenamiento. La función debe crear un modelo, compilarlo para _warming up_, realizar el _warming up_, y luego compilar el modelo de nuevo para _fine tuning_ (liberando los pesos de todas las capas), y entrenar de nuevo.\n",
        "\n",
        "**Entrada**:\n",
        "\n",
        "*    **`X_train`**: un objeto tipo `BatchEncoding` para tokenizar textos.\n",
        "*    **`y_train`**: un arreglo de numpy `np.array` que representa las etiquetas de los datos de entrenamiento.\n",
        "*    **`X_val`**: un objeto tipo `BatchEncoding` para tokenizar textos.\n",
        "*    **`y_val`**: un arreglo de numpy `np.array` que representa las etiquetas de los datos de validación.\n",
        "*    **`l_r_warming_up`**: `float`, la tasa de aprendizaje a usar durante el calentamiento.\n",
        "*    **`epochs_warming_up`**: `int`, un número entero que representa el número de épocas de entrenamiento usadas en el calentamiento.\n",
        "*    **`l_r_fine_tuning`**: `float`, la tasa de aprendizaje a usar durante el _fine tuning_.\n",
        "*    **`epochs_fine_tuning`**: `int`, un número entero que representa el número de épocas de entrenamiento usadas en el _fine tuning_.\n",
        "\n",
        "**Salida**:\n",
        "\n",
        "*  **`history`**: un objeto tipo `History` de tensorflow con la información del entrenamiento del modelo.\n",
        "*    **`model`**: una instancia del modelo entrenado tipo `TFAutoModelForSequenceClassification`\n",
        "\n",
        "> **Notas**:\n",
        "  * Debe definir el modelo usando la función `pretrained_model`, cargando `'distilbert-base-uncased'`.\n",
        "  * Debe compilar el modelo dos veces usando la función `compile_model`. La primera vez será para configurar el modelo para el _warming up_. La seguda vez que compile será después de haber hecho el _warming up_, para habilitar el entrenamiento de todas las capas y cambiar la tasa de aprendizaje.\n",
        "  * Analogamente, debe usar dos veces la función `train_model`, una vez para _warming up_, y la segunda vez para _fine tuning_.\n",
        "  * `tf.keras.optimizers.Adam()` debe ser el optimizador de todos los entrenamientos.\n",
        "  * La función debe usar un `batch_size` de 32 en todos los entrenamientos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yDo3-MSLfmFg",
      "metadata": {
        "id": "yDo3-MSLfmFg"
      },
      "outputs": [],
      "source": [
        "# FUNCION CALIFICADA fine_tuning\n",
        "def fine_tuning(X_train, y_train,\n",
        "                    X_val, y_val,\n",
        "                    l_r_warming_up,\n",
        "                    epochs_warming_up,\n",
        "                    l_r_fine_tuning,\n",
        "                    epochs_fine_tuning):\n",
        "\n",
        "    model = pretrained_model(\"distilbert-base-uncased\")\n",
        "\n",
        "    # Compilar el modelo para el warming up\n",
        "    model = compile_model(model, optimizer=tf.keras.optimizers.Adam(learning_rate=l_r_warming_up), l_r=l_r_warming_up)\n",
        "\n",
        "    # Entrenar el modelo para el warming up\n",
        "    history_warming_up, _ = train_model(model, X_train, y_train, X_val, y_val,\n",
        "                                         epochs=epochs_warming_up, batch_size=32, train_base=False)\n",
        "\n",
        "    # Definir el modelo para el fine tuning\n",
        "    model = pretrained_model(\"distilbert-base-uncased\")\n",
        "\n",
        "    # Compilar el modelo para el fine tuning\n",
        "    model = compile_model(model, optimizer=tf.keras.optimizers.Adam(learning_rate=l_r_fine_tuning), l_r=l_r_fine_tuning)\n",
        "\n",
        "    # Entrenar el modelo para el fine tuning\n",
        "    history_fine_tuning, trained_model = train_model(model, X_train, y_train, X_val, y_val,\n",
        "                                                     epochs=epochs_fine_tuning, batch_size=32, train_base=True)\n",
        "\n",
        "\n",
        "    return history_fine_tuning, trained_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hYmFXsvQBx9F",
      "metadata": {
        "id": "hYmFXsvQBx9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b88cef-e17c-43ee-fb02-a4f9f8afe086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "501/501 [==============================] - 157s 254ms/step - loss: 0.6941 - val_loss: 0.6870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "501/501 [==============================] - 159s 257ms/step - loss: 0.2801 - val_loss: 0.2120\n",
            "Model: \"tf_distil_bert_for_sequence_classification_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  66362880  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            " dropout_275 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66955010 (255.41 MB)\n",
            "Trainable params: 66955010 (255.41 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "learnign_rate = 4.999999873689376e-05\n",
            "dict_keys(['loss', 'val_loss'])\n",
            "El modelo se ha entrenado durante 1 epoch\n"
          ]
        }
      ],
      "source": [
        "# TEST_CELL\n",
        "history, model = fine_tuning(X_train=train_encodings,\n",
        "                              y_train=np.asarray(y_train),\n",
        "                              X_val=val_encodings,\n",
        "                              y_val=np.asarray(y_val),\n",
        "                              l_r_warming_up=1e-3,\n",
        "                              epochs_warming_up=1,\n",
        "                              l_r_fine_tuning=5e-5,\n",
        "                              epochs_fine_tuning=1)\n",
        "print(model.summary())\n",
        "print('learnign_rate =',model.get_compile_config()['optimizer']['config']['learning_rate'])\n",
        "print(history.history.keys())\n",
        "print('El modelo se ha entrenado durante',len(history.history['val_loss']),'epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NcfVMuPHBpu0",
      "metadata": {
        "id": "NcfVMuPHBpu0"
      },
      "source": [
        "**Salida esperada:**\n",
        "> **Nota**: los valores de `loss` y `val_loss` pueden cambiar.\n",
        "\n",
        "```\n",
        "501/501 [==============================] - 147s 226ms/step - loss: 0.6918 - val_loss: 0.6861\n",
        "501/501 [==============================] - 54s 95ms/step - loss: 0.6859 - val_loss: 0.6859\n",
        "Model: \"tf_distil_bert_for_sequence_classification_1\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " distilbert (TFDistilBertMai  multiple                 66362880  \n",
        " nLayer)                                                         \n",
        "                                                                 \n",
        " pre_classifier (Dense)      multiple                  590592    \n",
        "                                                                 \n",
        " classifier (Dense)          multiple                  1538      \n",
        "                                                                 \n",
        " dropout_39 (Dropout)        multiple                  0         \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 66,955,010\n",
        "Trainable params: 66,955,010\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "None\n",
        "learnign_rate = 4.999999873689376e-05\n",
        "dict_keys(['loss', 'val_loss'])\n",
        "El modelo se ha entrenado durante 1 epoch\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QVoYfKroEAJn",
      "metadata": {
        "id": "QVoYfKroEAJn"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iId0fMP0jbAG",
      "metadata": {
        "id": "iId0fMP0jbAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c70c5e-9750-49b2-ee26-9aa775b38c7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [0.28006845712661743], 'val_loss': [0.2120366096496582]}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "## TEACHEAR\n",
        "history.history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4KDwN-ZFx4ik",
      "metadata": {
        "id": "4KDwN-ZFx4ik"
      },
      "source": [
        "> Tiempo estimado: 4:10:00 h sin GPU, 04:00 m con GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7vltS8gWp3c5",
      "metadata": {
        "id": "7vltS8gWp3c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "9d10a5c1-34fc-4ff0-adb4-f90ee34b5c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 2_4_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Test Run correctly</h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h6 style=\"color: green;\">Your test grade: 100</h6>\n",
              "<h6>Feedback: Probando código ejemplo.</h6>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "grader.run_test(\"Test 2_4_1\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aVP3vDm6vRKn",
      "metadata": {
        "id": "aVP3vDm6vRKn"
      },
      "source": [
        "## **2.5 Evaluar**\n",
        "---\n",
        "Complete la función `evaluate_model` que recible un modelo entrenado y lo evalua en un conjunto de prueba.\n",
        "\n",
        "**Entrada**:\n",
        "\n",
        "*  **`model`**: una instancia del modelo entrenado tipo `TFAutoModelForSequenceClassification`\n",
        "*    **`X_test`**: un objeto tipo `BatchEncoding` para tokenizar textos.\n",
        "*    **`y_test`**: un arreglo de numpy `np.array` que representa las etiquetas de los datos de entrenamiento.\n",
        "\n",
        "**Salida**:\n",
        "* **`report`**: un `str` dado por la función `classification_report` de Scikit-Learn.\n",
        "\n",
        "> Nota: recuerde aplicar `tf.math.softmax` a las predicciones del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nv6ktYTTZ_Jl",
      "metadata": {
        "id": "nv6ktYTTZ_Jl"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN CALIFICADA evaluate\n",
        "from sklearn.metrics import *\n",
        "def evaluate(model, X_test, y_test):\n",
        "    # Realizar predicciones en el conjunto de prueba\n",
        "    predictions = model.predict(dict(X_test))\n",
        "\n",
        "    # Aplicar softmax a las predicciones\n",
        "    predictions = tf.math.softmax(predictions.logits)\n",
        "\n",
        "    # Obtener las clases predichas\n",
        "    y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "\n",
        "    # Generar el reporte de clasificación\n",
        "    report = classification_report(y_test, y_pred)\n",
        "\n",
        "    return report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "L7I3JxY8BzSg",
      "metadata": {
        "id": "L7I3JxY8BzSg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5228cadd-5f86-400c-d1fb-28c0013d447e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "167/167 [==============================] - 13s 77ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.87      0.92      2997\n",
            "           1       0.85      0.96      0.90      2345\n",
            "\n",
            "    accuracy                           0.91      5342\n",
            "   macro avg       0.91      0.92      0.91      5342\n",
            "weighted avg       0.92      0.91      0.91      5342\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TEST_CELL\n",
        "print(evaluate(model=model,\n",
        "               X_test=test_encodings,\n",
        "               y_test=y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fk0ryES4j62",
      "metadata": {
        "id": "1fk0ryES4j62"
      },
      "source": [
        "\n",
        "```\n",
        "167/167 [==============================] - 13s 65ms/step\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.56      1.00      0.72      2997\n",
        "           1       0.00      0.00     yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
        "    accuracy                           0.56      5342\n",
        "   macro avg       0.28      0.50      0.36      5342\n",
        "weighted avg       0.31      0.56      0.40      5342\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lyryG_KlEB6Y",
      "metadata": {
        "id": "lyryG_KlEB6Y"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6TFGQe_jPRGk",
      "metadata": {
        "id": "6TFGQe_jPRGk"
      },
      "source": [
        "> Tiempo estimado: 1:00 m sin GPU, 00:29 m con GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "ezHjXU3mINRx",
      "metadata": {
        "id": "ezHjXU3mINRx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "617bba07-994d-476b-a6c4-7b9acbbf7ec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 2_5_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Test Run correctly</h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h6 style=\"color: green;\">Your test grade: 100</h6>\n",
              "<h6>Feedback: Probando código ejemplo.</h6>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "grader.run_test(\"Test 2_5_1\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4yNMUWeUqwMF",
      "metadata": {
        "id": "4yNMUWeUqwMF"
      },
      "source": [
        "## **Entrenando durante más _epochs_**\n",
        "----\n",
        "En este punto usted ha debido programar con éxito todos los pasos necesairios para hacer _Fine Tuning_ con _DistilBERT_ en usando _HuggingFace_, pero no hemos entrenado lo suficiente el modelo. A continuación use su código para el entrenar el modelo durante algunas epochs de calentamiento y otras más de _fine tuning_, cambiando en cada caso las tasas de aprendizaje y evaluando los resultados.\n",
        "\n",
        "> **Nota: este código no es calificable.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yahBNV4RqxIF",
      "metadata": {
        "id": "yahBNV4RqxIF"
      },
      "outputs": [],
      "source": [
        "history, model = fine_tuning(X_train=train_encodings,\n",
        "                              y_train=np.asarray(y_train),\n",
        "                              X_val=val_encodings,\n",
        "                              y_val=np.asarray(y_val),\n",
        "                              l_r_warming_up=5e-3,\n",
        "                              epochs_warming_up=5,\n",
        "                              l_r_fine_tuning=5e-5,\n",
        "                              epochs_fine_tuning=20)\n",
        "print(evaluate(model=model,\n",
        "               X_test=test_encodings,\n",
        "               y_test=np.array(y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MKw7uZ33EMIS",
      "metadata": {
        "id": "MKw7uZ33EMIS"
      },
      "source": [
        "# **Evaluación**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ViRJ8e5vPbbT",
      "metadata": {
        "id": "ViRJ8e5vPbbT"
      },
      "source": [
        "> Tiempo estimado: 06:00:00 h sin GPU, 10:30 m con GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jgeaNWRhEOSc",
      "metadata": {
        "id": "jgeaNWRhEOSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f86c395c-c3de-48e3-e351-42d853807e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1_1_1\n",
            "Test 1_1_2\n",
            "Test 2_1_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 2_1_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 2_2_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 2_2_2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 2_3_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "grader.submit_task(globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_eRlyPGbJtsJ",
      "metadata": {
        "id": "_eRlyPGbJtsJ"
      },
      "source": [
        "# **Referencias**\n",
        "---\n",
        "* [*Fine-tuning with custom datasets*](https://huggingface.co/transformers/v3.2.0/custom_datasets.html)\n",
        "\n",
        "* [*Transformer Models For Custom Text Classification Through Fine-Tuning*](https://towardsdatascience.com/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1)\n",
        "\n",
        "* [*Fine-Tuning Hugging Face Model with Custom Dataset*](https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333)\n",
        "\n",
        "* [*Fine-Tuning Bert for Tweets Classification ft. Hugging Face*](https://medium.com/mlearning-ai/fine-tuning-bert-for-tweets-classification-ft-hugging-face-8afebadd5dbf)\n",
        "\n",
        "* [*Coronavirus tweets NLP - Text Classification*](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification?resource=download&select=Corona_NLP_test.csv)\n",
        "\n",
        "* [*covid_tweet_classification.ipynb*](https://colab.research.google.com/github/codistro/Articles/blob/main/covid_tweet_classification.ipynb#scrollTo=bmeDURiI7Q6M)\n",
        "\n",
        "\n",
        "* _Origen de imágenes_\n",
        "\n",
        "  - Swatimeena. (2021, 16 diciembre). DistilBERT Text classification using Keras - Swatimeena - Medium. Medium. [Imagen] https://miro.medium.com/v2/resize:fit:720/format:webp/1*e5G4Vdt6gSFkRjs3fD36Pg.png\n",
        "\n",
        "  - Davaadorj,M. (2022, 19 marzo).\n",
        "Huggingface-course: Documentation-images - Full Nlp Pipeline [Imagen] https://huggingface.co/datasets/huggingface-course/documentation-images/blob/main/en/chapter2/full_nlp_pipeline.svg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "APRHBmlSxKGo",
      "metadata": {
        "id": "APRHBmlSxKGo"
      },
      "source": [
        "# **Créditos**\n",
        "---\n",
        "\n",
        "* **Profesor:** [Fabio Augusto Gonzalez](https://dis.unal.edu.co/~fgonza/)\n",
        "* **Asistentes docentes :**\n",
        "  * [Santiago Toledo Cortés](https://sites.google.com/unal.edu.co/santiagotoledo-cortes/)\n",
        "* **Diseño de imágenes:**\n",
        "    - [Mario Andres Rodriguez Triana](mailto:mrodrigueztr@unal.edu.co).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}